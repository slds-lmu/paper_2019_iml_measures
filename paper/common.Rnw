% sets
\renewcommand{\S}{S}
\newcommand{\Spi}[1][j]{B_{#1} (\pi)}
\newcommand{\Scpi}[1][j]{A_{#1} (\pi)}
\newcommand{\Sc}{C}
\newcommand{\Sj}{\S \cup \{j\}}
\newcommand{\pairs}[1]{\{ (#1) \}}

\newcommand{\Lswitch}[1][\Xperm_{\S}]{L (\fh (#1, \XC ), Y)}
\newcommand{\Eswitch}{\E (\Lswitch)} %\E_{\Xperm_\S}(GE(\fh(\Xperm_\S, X_{\Sc})))
\newcommand{\Eorig}{\E (L (\fh (X), Y)) } %GE(\fh(X_\S, X_{\Sc}))

% math spaces
\newcommand{\N}{\mathds{N}}                                                 % N, naturals
\newcommand{\Z}{\mathds{Z}}                                                 % Z, integers
\newcommand{\Q}{\mathds{Q}}                                                 % Q, rationals
\newcommand{\R}{\mathds{R}}                                                 % R, reals
\newcommand{\C}{\mathds{C}}                                                 % C, complex

% basic math stuff
\def\argmax{\mathop{\sf arg\,max}}                                          % argmax
\def\argmin{\mathop{\sf arg\,min}}                                          % argmin
\newcommand{\sign}{\operatorname{sign}}                                     % sign, signum
\newcommand{\I}{\mathbb{I}}                                                 % I, indicator
\newcommand{\order}{\mathcal{O}}                                            % O, order
\newcommand{\fp}[2]{\frac{\partial #1}{\partial #2}}                        % partial derivative
\newcommand{\continuous}{\mathcal{C}}                                       % C, space of continuous functions
\newcommand{\distas}[1]{\overset{#1}{\sim}}                                 % ????????????
\newcommand{\pd}[2]{\frac{\partial{#1}}{\partial #2}}
\newcommand{\sumin}{\sum_{i=1}^n}
\newcommand{\sumkg}{\sum_{k=1}^g}
\newcommand{\prodin}{\prod_{i=1}^n}
\newcommand{\prodkg}{\prod_{k=1}^g}
\newcommand{\xt}{\tilde x}													                        % x tilde
\newcommand{\cupdot}{\mathbin{\mathaccent\cdot\cup}}

% linear algebra
\newcommand{\one}{\boldsymbol{1}}                                           % 1, unitvector
\newcommand{\id}{\mathrm{I}}                                                % I, identity
\newcommand{\diag}{\operatorname{diag}}                                     % diag, diagonal
\newcommand{\trace}{\operatorname{tr}}                                      % tr, trace
\newcommand{\spn}{\operatorname{span}}                                      % span
\newcommand{\scp}[2]{\left\langle #1, #2 \right\rangle}                     % <.,.>, scalarproduct
\let\myxi\xi																% xi, slack variable (SVM)

% basic probability + stats
\renewcommand{\P}{\mathds{P}}                                               % P, probability
\newcommand{\E}{\mathds{E}}                                                 % E, expectation
\newcommand{\var}{\mathsf{Var}}                                             % Var, variance
\newcommand{\cov}{\mathsf{Cov}}                                             % Cov, covariance
\newcommand{\corr}{\mathsf{Corr}}                                           % Corr, correlation
\newcommand{\normal}{\mathcal{N}}                                           % N of the normal distribution
\newcommand{\iid}{\overset{i.i.d}{\sim}}                                    % dist with i.i.d superscript


% machine learning

%%%%%% ml - data
\newcommand{\Xspace}[1][P]{\mathcal{X}_{#1}}                                           % X, input space
\newcommand{\Yspace}{\mathcal{Y}}                                           % Y, output space
\newcommand{\X}{\mathbf{X}}
\newcommand{\nset}{\{1, \ldots, n\}}                                        % set from 1 to n
\newcommand{\pset}{\{1, \ldots, p\}}                                        % set from 1 to p
\newcommand{\gset}{\{1, \ldots, g\}}                                        % set from 1 to g
\newcommand{\Pxy}{\P_{xy}}                                                  % P_xy
% observation (x, y)
\newcommand{\xvec}{(x_1, \ldots, x_p)^\top}                                 % (x1, ..., xp)
\newcommand{\xivec}{(x^{(i)}_1, \ldots, x^{(i)}_p)^\top}                    % (x1^i, ..., xp^i)
\renewcommand{\xi}[1][(i)]{\mathbf{x}^{#1}}                                          % x^i
\newcommand{\xis}[1][(i)]{\mathbf{x}_{\S}^{#1}}                                          % x^i
\newcommand{\zi}[1][(i)]{\mathbf{z}^{#1}}
\newcommand{\zis}[1][(i)]{\mathbf{z}_{\S}^{#1}}                                          % x^i
\newcommand{\zic}[1][(i)]{\mathbf{z}_{\Sc}^{#1}}                                          % x^i
\newcommand{\xic}[1][(i)]{\mathbf{x}_{\Sc}^{#1}}                                          % x^i
\newcommand{\xij}{x_j^{(i)}}
\newcommand{\yi}[1][i]{y^{(#1)}}                                            % y^i
\newcommand{\xyi}{(\xi, \yi)}                                               % (x^i, y^i)
\newcommand{\xy}{(x, y)}                                                    % (x, y)
% data
\newcommand{\D}{\mathcal{D}}                                                % D, data
%\newcommand{\Dset}{\{ (\mathbf{x}^{(1)}, y^{(1)}), \ldots, (\mathbf{x}^{(n)},  y^{(n)})\}}    % {(x1,y1)), ..., (xn,yn)} data
\newcommand{\Dset}[1][n]{\pairs{ \mathbf{x}^{(i)}, y^{(i)} }_{i=1}^{#1}}    % {(x1,y1)), ..., (xn,yn)} data
\newcommand{\xdat}{\{ x^{(1)}, \ldots, x^{(n)}\}}   						            % {x1, ..., xn} data
\newcommand{\Dtrain}{\mathcal{D}_{\text{train}}}                            % D_train, training set
\newcommand{\Dtest}{\D}%{\mathcal{T}}%{\mathcal{D}_{\text{test}}}                              % D_test, test set
\newcommand{\xj}{\mathbf{x}_j}                                              % x_j (bold)
\newcommand{\xone}{\mathbf{x}_1}                                              % x_j (bold)
\newcommand{\xp}{\mathbf{x}_p}                                              % x_j (bold)
\newcommand{\xS}{\mathbf{x}_S}                                              % x_S (bold)
\newcommand{\xC}{\mathbf{x}_{\Sc}}                                              % x_S (bold)
\newcommand{\Xperm}{\tilde{X}}
\newcommand{\XC}{X_{\Sc}}
\newcommand{\XS}{X_{\S}}
\newcommand{\xjvec}{(x^{(1)}_j, \ldots, x^{(n)}_j)^\top}                    % (x^1_j, ..., x^p_j)
\newcommand{\ydat}{\mathbf{y}}                                              % y (bold)
\newcommand{\yvec}{(y^{(1)}, \hdots, y^{(n)})^\top}                         % (y1, ..., yn)


%%%%%% ml - model
% prediction function f, theta, yhat
\newcommand{\Hspace}{H}
\newcommand{\fx}{f(x)}                                                      % f(x)
\newcommand{\fxh}{\fh(\mathbf x)}                                                   % fhat(x)
\newcommand{\fxt}{f(x | \theta)}                                            % f(x | theta)
\newcommand{\fxi}{f(\xi)}                                                   % f(x^(i))
\newcommand{\fxih}{\hat{f}(\xi)}                                              % f(x^(i))
\newcommand{\fxit}{f(x^{(i)} | \theta)}                                     % f(x^(i) | theta)
\newcommand{\fh}{\hat{f}}                                                   % f hat
\newcommand{\fhS}{\fh_{\S}}
\newcommand{\fS}{f_{\S}}
\newcommand{\fhSi}{\fhS^{(i)}}
\newcommand{\thetah}{\hat{\theta}}                                          % theta hat
\newcommand{\fhD}{\fh_{\D}}                                                 %
\newcommand{\fhDtrain}{\fh_{\Dtrain}}                                       %

\newcommand{\MR}{PFI}
\newcommand{\hMR}{\widehat{\MR}}
\newcommand{\MRS}[1][\S]{\MR_{#1}}
\newcommand{\hMRS}[1][\S]{\hMR_{#1}}
\newcommand{\hMRSa}[1][\S]{\hMR_{#1, \text{approx}}}
%\newcommand{\w}[1][\S]{v_{#1}(\fh, \Dtest)}
\newcommand{\w}[1][(\S)]{v_{GE}#1}
%\newcommand{\Lii}{\Li[(i)]} % L(\fh(\xi), \yi)
%\newcommandx{\Li}[2][1=(k), 2=(i)]{L^{#2}(\fh, \xis[#1])}

\newcommand{\Lii}{L(\fh(\xi), \yi)}
\newcommandx{\Li}[3][1=(k), 2=(i), 3=(i)]{L(\fh(\xis[#1], \xic[#2]), y^{#3})}

\newcommand{\mfeat}{m_{\textnormal{feat}}}
\newcommand{\mobs}{m_{\textnormal{obs}}}

\newcommand{\hPI}{\widehat{PI}_{\S}} %  \widehat{\MR}_{\S}
\newcommand{\PI}[1][\S]{PI_{#1}}
\newcommand{\ICI}{\hMRS^{(i)}}
\newcommand{\ICIvar}[1][(i)]{\MRS^{#1}}
%\newcommandx{\PFIikS}[3][1=\xi, 2=\mathbf{x}_{\S}^{(k)}, 3=\S]{PFI_{#3} (#1, #2)}
%\newcommand{\PFIikS}[1][(k)]{\widehat{\MR}^{(i)} (\xis[#1])} % was used until 29.3
\newcommandx{\PFIikS}[2][1=(i), 2=(k)]{\Delta L^{#1} (\xis[#2])}
%\newcommand{\PFIikS}{\widehat{\MR}^{(i)} (\xis[(k)])}
\newcommand{\yh}{\hat{y}}                                                   % y hat for prediction of target
\newcommand{\yih}{\hat{y}}                                                  % y hat for prediction of target
% prediction function h
\newcommand{\hx}{h(x)}                                                      % h(x)
\newcommand{\hxt}{h(x | \theta)}                                            % h(x | theta)
\newcommand{\hxi}{h(\xi)}                                                   % h(x^(i))
\newcommand{\hxit}{h(x^{(i)} | \theta)}                                     % h(x^(i) | theta)
\newcommand{\hh}{\hat{h}}                                                   % h hat
\newcommand{\hxh}{\hat{h}(x)}                                               % hhat(x)
% risk
% pdf of x and (x,y)
\newcommand{\post}{\P(y = 1 | x)}                                           % P(y = 1 | x), post. prob for y=1
\newcommand{\postk}{\P(y = k | x)}                                          % P(y = k | y), post. prob for y=k
\newcommand{\pik}{\pi_k}                                                    % pi_k, prior
\newcommand{\pix}{\pi(x)}                                                   % pi(x), P(y = 1 | x)
\newcommand{\pixt}{\pi(x | \theta)}                                          %
\newcommand{\pikx}{\pi_k(x)}                                                % pi_k(x), P(y = k | x)
\newcommand{\pikxt}{\pi_k(x | \theta)}                                       %
\newcommand{\pijx}{\pi_j(x)}                                                % pi_j(x), P(y = j | x)
\newcommand{\pixh}{\hat \pi(x)}                                             % pi(x) hat, P(y = 1 | x) hat
\newcommand{\pikxh}{\hat \pi_k(x)}                                          % pi_k(x) hat, P(y = k | x) hat
\newcommand{\pdf}{p}                                                        % p
\newcommand{\pdfx}{p(x)}                                                    % p(x)
\newcommand{\pdfxy}{p(x,y)}                                                 % p(x, y)
\newcommand{\pdfxyt}{p(x, y | \theta)}                                      % p(x, y | theta)
\newcommand{\pdfxyit}{p(\xi, \yi | \theta)}                                 % p(x^(i), y^(i) | theta)
\newcommand{\pdfxyk}{p(x | y=k)}                                            % p(x | y = k)
\newcommand{\pdfxiyk}{p(\xi | y=k)}                                         % p(x^i | y = k)
\newcommand{\lpdfxyk}{\log \pdfxyk}                                         %
\newcommand{\lpik}{\log \pik}                                               %
\newcommand{\pdfygxt}{p(y |x, \theta)}                                      % p(y | x, theta)
\newcommand{\pdfyigxit}{p(\yi |\xi, \theta)}                                % p(y^i |x^i, theta)
\newcommand{\lpdfygxt}{\log \pdfygxt }                                      %
\newcommand{\lpdfyigxit}{\log \pdfyigxit}                                   %

% residual and margin
\newcommand{\eps}{\epsilon}                                                 % residual, stochastic
\newcommand{\epsi}{\epsilon^{(i)}}                                          % r^i, residual, stochastic
\newcommand{\epsh}{\hat{\epsilon}}                                          % residual, estimated
\newcommand{\yf}{y \fx}                                                     % y f(x), margin
\newcommand{\yfi}{\yi \fxi}                                                 % y^i f(x^i), margin
\newcommand{\Sigmah}{\hat \Sigma}									                      		% estimated covariance matrix
\newcommand{\Sigmahj}{\hat \Sigma_j}									                    	% estimated covariance matrix for the j-th class


% ml - loss, risk, likelihood
\newcommand{\Lxy}{L(y, f(x))}                                               % L(y, f(x))
\newcommand{\Lxyi}{L(\yi, \fxi)}                                            % L(y^i, f(x^i))
\newcommand{\Lxyt}{L(y, \fxt)}                                              % L(y, f(x | theta))
\newcommand{\Lxyit}{L(\yi, \fxit)}                                          % L(y^i, f(x^i | theta)
\newcommand{\risk}{\mathcal{R}}                                             % R
\newcommand{\riskf}{\risk(f)}                                               % R(f)
\newcommand{\riske}{\mathcal{R}_{\text{emp}}}                               % R_emp
\newcommand{\riskef}{\riske(f)}                                             % R_emp(f)
\newcommand{\risket}{\mathcal{R}_{\text{emp}}(\theta)}                      % R_emp(theta)
\newcommand{\riskr}{\mathcal{R}_{\text{reg}}}                               % R_reg
\newcommand{\riskrt}{\mathcal{R}_{\text{reg}}(\theta)}                      % R_reg(theta)
\newcommand{\riskrf}{\riskr(f)}                                             % R_reg(f)
\newcommand{\LL}{\mathcal{L}}                                               % L, likelihood
\newcommand{\LLt}{\mathcal{L}(\theta)}                                      % L(theta), likelihood
\renewcommand{\ll}{\ell}                                                    % l, log-likelihood
\newcommand{\llt}{\ell(\theta)}                                             % l(theta), log-likelihood
\newcommand{\LS}{\mathfrak{L}}                                              % ????????????
\newcommand{\TS}{\mathfrak{T}}                                              % ??????????????
\newcommand{\errtrain}{\text{err}_{\text{train}}}                           % training error
\newcommand{\errtest}{\text{err}_{\text{test}}}                             % training error
\newcommand{\errexp}{\overline{\text{err}_{\text{test}}}}                   % training error

%ml - trees, extra trees
\newcommand{\Np}{\mathcal{N}}												                        % Parent node N
\newcommand{\Nl}{\Np_1}														                          % Left node N_1
\newcommand{\Nr}{\Np_2}														                          % Right node N_2


%ml - bagging, rF, boosting
\newcommand{\bl}[1]{b^{[#1]}(x)}												                    % baselearner w. argument for m
\newcommand{\blm}{\bl{m}}												                            % baselearner w. argument for m
\newcommand{\blmh}{\hat{b}^{[m]}(x)}
% ml - svms
\newcommand{\sv}{\operatorname{SV}}                                         % supportvectors
\newcommand{\HS}{\mathcal{H}}                                               % H, hilbertspace

% ml - boosting
\newcommand{\fm}{f^{[m]}}                                                   % prediction in iteration m
\newcommand{\fmh}{\hat{f}^{[m]}}                                                   % prediction in iteration m
\newcommand{\fmd}{f^{[m-1]}}                                                % prediction m-1
\newcommand{\fmdh}{\hat{f}^{[m-1]}}                                                % prediction m-1
\newcommand{\bmm}{b^{[m]}}                                                  % basemodel m
\newcommand{\bmmh}{\hat{b}^{[m]}}                                           % basemodel m with hat
\newcommand{\betam}{\beta^{[m]}}                                            % weight of basemodel m
\newcommand{\betamh}{\hat{\beta}^{[m]}}                                     % weight of basemodel m with hat
\newcommand{\betai}[1]{\beta^{[#1]}}                                        % weight of basemodel with argument for m
\newcommand{\errm}{\text{err}^{[m]}}                                        % weighted in-sample misclassification rate
\newcommand{\wm}{w^{[m]}}                                                   % weight vector of basemodel m
\newcommand{\wmi}{w^{[m](i)}}                                               % weight of obs i of basemodel m
\newcommand{\thetam}{\theta^{[m]}}                                          % parameters of basemodel m
\newcommand{\thetamh}{\hat{\theta}^{[m]}}                                   % parameters of basemodel m with hat
\newcommand{\rmm}{r^{[m]}}                                                  % pseudo residuals
\newcommand{\rmi}{r^{[m](i)}}                                               % pseudo residuals
\newcommand{\Rtm}{R_{t}^{[m]}}                                              % terminal-region
\newcommand{\Tm}{T^{[m]}}                                                   %
\newcommand{\ctm}{c_t^{[m]}}                                                % mean, terminal-regions
\newcommand{\ctmh}{\hat{c}_t^{[m]}}                                         % mean, terminal-regions with hat
\newcommand{\ctmt}{\tilde{c}_t^{[m]}}                                       % mean, terminal-regions
\newcommand{\fxk}{f_k(x)}                                                   % f_k(x)
\newcommand{\Lp}{L^\prime}
\newcommand{\Ldp}{L^{\prime\prime}}
\newcommand{\Lpleft}{\Lp_{\text{left}}}

% ml - NNs
\newcommand{\neurons}{z_1,\dots,z_M} % vector of neurons

% resampling
\newcommand{\GE}[1]{GE(\fh_{#1})}                                           % Generalization error GE
\newcommand{\GEh}[1]{\widehat{GE}_{#1}}                                     % Estimated train error
\newcommand{\GED}{\GE{\D}}                                                  % Generalization error GE
\newcommand{\EGEn}{EGE_n}                                                  % Generalization error GE
\newcommand{\EDn}{\E_{|\D| = n}}                                                  % Generalization error GE

% irace ????????????
\newcommand{\costs}{\mathcal{C}} % costs
\newcommand{\Celite}{\theta^*} % elite configurations
\newcommand{\instances}{\mathcal{I}} % sequence of instances
\newcommand{\budget}{\mathcal{B}} % computational budget

% basic latex stuff
\newcommand{\pkg}[1]{{\fontseries{b}\selectfont #1}} %fontstyle for R packages
\newcommand{\lz}{\vspace{0.5cm}} %vertical space
\newcommand{\dlz}{\vspace{1cm}} %double vertical space
\newcommand{\mat}[1]{ %short pmatrix command
  \begin{pmatrix}
    #1
  \end{pmatrix}
}
\newcommand{\oneliner}[1] % Oneliner for important statements
{\begin{block}{}\begin{center}\begin{Large}#1\end{Large}\end{center}\end{block}}

% MBO
\newcommand{\sxh}{\hat{s}(x)} % uncertainty shat(x)
\newcommand{\vxh}{\hat{s}^2(x)}
% MBO - eventuell Ã¼berarbeiten / mit SVM abgleichen
\newcommand{\matK}{\mathbf{K}}
\newcommand{\kstarx}{\mathbf{k}_*(x)}
\newcommand{\xpi}[1][i]{x^{*(#1)}}
\newcommand{\yv}{\boldsymbol{y}}

\newcommand{\new}[1]{{#1}} %\newcommand{\new}[1]{{\color{red} #1}}
\newcommand{\code}[1]{\texttt{#1}}

\makeatletter
\newcommand*{\overtabline}{%
  \noalign{%
    % normal "baselineskip" in tabular is height + depth of \@arstrutbox
    \vskip-.5\dimexpr\ht\@arstrutbox+\dp\@arstrutbox\relax
    % default line thickness is 0.4pt
    \vskip-.2pt\relax
    \hrule
    \vskip-.2pt\relax
    \vskip+.5\dimexpr\ht\@arstrutbox+\dp\@arstrutbox\relax
  }%
}

\newcounter{mycomment}
\newcommand{\mycomment}[2][]{%
    % initials of the author (optional) + note in the margin
    \refstepcounter{mycomment}%
    {%
    \todo[color={red!100!green!33},size=\small]{%
        \textbf{Comment [\uppercase{#1}\themycomment]:}~#2}%
    }}
\newcommand{\todoBH}[1]{\mycomment[BH]{#1}}

